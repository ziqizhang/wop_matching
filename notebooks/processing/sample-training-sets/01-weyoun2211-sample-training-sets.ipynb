{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Load the \"autoreload\" extension so that code can change\n",
    "%load_ext autoreload\n",
    "\n",
    "# OPTIONAL: always reload modules so that as you change code in src, it gets loaded\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/rpeeters/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from os.path import isdir, join\n",
    "from os.path import basename\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from itertools import combinations\n",
    "from itertools import product\n",
    "\n",
    "import textdistance\n",
    "import copy\n",
    "import os\n",
    "import math\n",
    "import string\n",
    "import tqdm\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from itertools import repeat\n",
    "\n",
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand</th>\n",
       "      <th>category</th>\n",
       "      <th>cluster_id</th>\n",
       "      <th>description</th>\n",
       "      <th>id</th>\n",
       "      <th>identifiers</th>\n",
       "      <th>keyValuePairs</th>\n",
       "      <th>price</th>\n",
       "      <th>specTableContent</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rockwell automation</td>\n",
       "      <td>Other_Electronics</td>\n",
       "      <td>3184606</td>\n",
       "      <td>cable between pcmk and plc5 or slc5 04 in dhpl...</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'/mpn': '[1784pcm5]'}]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>cable between pcmk and plc5 or slc5 04 in dhpl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>Jewelry</td>\n",
       "      <td>4865031</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>[{'/sku': '[15009616]'}]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2pcs golden lock and key ear cuff earring free...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>Grocery_and_Gourmet_Food</td>\n",
       "      <td>4865878</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>[{'/gtin13': '[7890557402128]'}]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>havaianas flash hit rose 33 34 pod product ope...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>Other_Electronics</td>\n",
       "      <td>1028988</td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td>[{'/sku': '[34852050]'}, {'/mpn': '[f5e10c10m1...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>panduit lc keyed c green to 50 125 om2 1 6mm m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>icebreaker</td>\n",
       "      <td>Camera_and_Photo</td>\n",
       "      <td>11784767</td>\n",
       "      <td>descripci n icebreaker kids compass l s half z...</td>\n",
       "      <td>4</td>\n",
       "      <td>[{'/sku': '[135889120]'}]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>icebreaker kids compass l s half zip boy compr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  brand                  category  cluster_id  \\\n",
       "id                                                              \n",
       "0   rockwell automation         Other_Electronics     3184606   \n",
       "1                                         Jewelry     4865031   \n",
       "2                        Grocery_and_Gourmet_Food     4865878   \n",
       "3                               Other_Electronics     1028988   \n",
       "4            icebreaker          Camera_and_Photo    11784767   \n",
       "\n",
       "                                          description  id  \\\n",
       "id                                                          \n",
       "0   cable between pcmk and plc5 or slc5 04 in dhpl...   0   \n",
       "1                                                       1   \n",
       "2                                                       2   \n",
       "3                                                       3   \n",
       "4   descripci n icebreaker kids compass l s half z...   4   \n",
       "\n",
       "                                          identifiers keyValuePairs price  \\\n",
       "id                                                                          \n",
       "0                            [{'/mpn': '[1784pcm5]'}]                       \n",
       "1                            [{'/sku': '[15009616]'}]                       \n",
       "2                    [{'/gtin13': '[7890557402128]'}]                       \n",
       "3   [{'/sku': '[34852050]'}, {'/mpn': '[f5e10c10m1...                       \n",
       "4                           [{'/sku': '[135889120]'}]                       \n",
       "\n",
       "   specTableContent                                              title  \n",
       "id                                                                      \n",
       "0                    cable between pcmk and plc5 or slc5 04 in dhpl...  \n",
       "1                    2pcs golden lock and key ear cuff earring free...  \n",
       "2                    havaianas flash hit rose 33 34 pod product ope...  \n",
       "3                    panduit lc keyed c green to 50 125 om2 1 6mm m...  \n",
       "4                    icebreaker kids compass l s half zip boy compr...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = pd.read_json('../../../data/raw/wdc-lspc/corpus/offers_corpus_english_v2.json.gz', lines=True)\n",
    "data_df.replace(np.nan, '', inplace=True)\n",
    "data_df = data_df.set_index('id', drop=False)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand</th>\n",
       "      <th>category</th>\n",
       "      <th>cluster_id</th>\n",
       "      <th>description</th>\n",
       "      <th>id</th>\n",
       "      <th>identifiers</th>\n",
       "      <th>keyValuePairs</th>\n",
       "      <th>price</th>\n",
       "      <th>specTableContent</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rockwell automation</td>\n",
       "      <td>Other_Electronics</td>\n",
       "      <td>3184606</td>\n",
       "      <td>cable between pcmk and plc5 or slc5 04 in dhpl...</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'/mpn': '[1784pcm5]'}]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>cable pcmk plc5 slc5 04 dhplus cp7 included 3d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>Jewelry</td>\n",
       "      <td>4865031</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>[{'/sku': '[15009616]'}]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2pcs golden lock key ear cuff earring free shi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>Grocery_and_Gourmet_Food</td>\n",
       "      <td>4865878</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>[{'/gtin13': '[7890557402128]'}]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>havaianas flash hit rose 33 34 pod product ope...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>Other_Electronics</td>\n",
       "      <td>1028988</td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td>[{'/sku': '[34852050]'}, {'/mpn': '[f5e10c10m1...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>panduit lc keyed c green 50 125 om2 1 6mm mult...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>icebreaker</td>\n",
       "      <td>Camera_and_Photo</td>\n",
       "      <td>11784767</td>\n",
       "      <td>descripci n icebreaker kids compass l s half z...</td>\n",
       "      <td>4</td>\n",
       "      <td>[{'/sku': '[135889120]'}]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>icebreaker kids compass l half zip boy comprar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  brand                  category  cluster_id  \\\n",
       "id                                                              \n",
       "0   rockwell automation         Other_Electronics     3184606   \n",
       "1                                         Jewelry     4865031   \n",
       "2                        Grocery_and_Gourmet_Food     4865878   \n",
       "3                               Other_Electronics     1028988   \n",
       "4            icebreaker          Camera_and_Photo    11784767   \n",
       "\n",
       "                                          description  id  \\\n",
       "id                                                          \n",
       "0   cable between pcmk and plc5 or slc5 04 in dhpl...   0   \n",
       "1                                                       1   \n",
       "2                                                       2   \n",
       "3                                                       3   \n",
       "4   descripci n icebreaker kids compass l s half z...   4   \n",
       "\n",
       "                                          identifiers keyValuePairs price  \\\n",
       "id                                                                          \n",
       "0                            [{'/mpn': '[1784pcm5]'}]                       \n",
       "1                            [{'/sku': '[15009616]'}]                       \n",
       "2                    [{'/gtin13': '[7890557402128]'}]                       \n",
       "3   [{'/sku': '[34852050]'}, {'/mpn': '[f5e10c10m1...                       \n",
       "4                           [{'/sku': '[135889120]'}]                       \n",
       "\n",
       "   specTableContent                                              title  \n",
       "id                                                                      \n",
       "0                    cable pcmk plc5 slc5 04 dhplus cp7 included 3d...  \n",
       "1                    2pcs golden lock key ear cuff earring free shi...  \n",
       "2                    havaianas flash hit rose 33 34 pod product ope...  \n",
       "3                    panduit lc keyed c green 50 125 om2 1 6mm mult...  \n",
       "4                    icebreaker kids compass l half zip boy comprar...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(s):\n",
    "    if isinstance(s, float):\n",
    "        if s != s:\n",
    "            return []\n",
    "    s = str(s)\n",
    "    s = s.replace('&amp;', \"\")\n",
    "    s = s.replace('&reg;', \"\")\n",
    "    s = s.replace('&quot;', \"\")\n",
    "    s = s.replace('\\t;', \" \")\n",
    "    s = s.replace('\\n;', \" \")\n",
    "    return s.lower().translate(str.maketrans('', '', string.punctuation)).split()\n",
    "\n",
    "def process_text(s, stop_words):\n",
    "    if isinstance(s, float):\n",
    "        if s != s:\n",
    "            return s\n",
    "    w_list = tokenize(s)\n",
    "    w_clean_list = [x for x in w_list if x not in stop_words]\n",
    "    string_final = \" \".join(w_clean_list)\n",
    "    return string_final\n",
    "\n",
    "\n",
    "stop_words_with_punct = copy.deepcopy(stopwords.words('english'))\n",
    "stop_words = list(map(lambda x: x.lower().translate(str.maketrans('', '', string.punctuation)), stop_words_with_punct))\n",
    "data_df['title'] = data_df['title'].apply(process_text, args=(stop_words,))\n",
    "\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clusters_for_gs_pairs(df, pairs):\n",
    "    gs_df = pd.read_json(pairs, lines=True)\n",
    "    offer_ids = set()\n",
    "    all_pairs = []\n",
    "    \n",
    "    ids = list(gs_df['pair_id'].values)\n",
    "    for x in ids:\n",
    "        split = x.split('#')\n",
    "        offer_ids.add(int(split[0]))\n",
    "        offer_ids.add(int(split[1]))\n",
    "        all_pairs.append([int(split[0]), int(split[1])])\n",
    "        \n",
    "    offers = df.loc[offer_ids]\n",
    "    clusters = offers['cluster_id'].unique()\n",
    "    return clusters, all_pairs, offers\n",
    "    \n",
    "\n",
    "def build_positive_pairs(df, gs, samples, set_label, clu_percent=1.0):\n",
    "    cat = basename(gs).replace('_gs.json.gz', '')\n",
    "    os.makedirs(os.path.dirname('../../../data/processed/build-training-sets/'+set_label+'/'+cat+'/positives/'), exist_ok=True)\n",
    "    try:\n",
    "        os.remove('../../../data/processed/build-training-sets/'+set_label+'/'+cat+'/positives/'+cat+'_train_positive'+'_'+set_label+'.txt')\n",
    "    except OSError:\n",
    "        pass\n",
    "    clusters, gs_pairs, gs_offers = get_clusters_for_gs_pairs(df, gs)\n",
    "    print('Clusters in '+cat+ ' ' + str(len(clusters)))\n",
    "    \n",
    "    if clu_percent != 1.0:\n",
    "        sample = random.sample(range(0, len(clusters)), k=int(round(clu_percent * len(clusters))))\n",
    "        clusters_new = [clusters[x] for x in sample]\n",
    "        clusters = clusters_new\n",
    "\n",
    "    no_clean_pair_cluster = 0\n",
    "    no_pair_at_all = 0\n",
    "    overall_pairs = []\n",
    "    \n",
    "    for cluster in clusters:\n",
    "        \n",
    "        cluster_offers = df[df['cluster_id'] == cluster]\n",
    "        cluster_offers = cluster_offers.dropna(subset=['title'])\n",
    "        cluster_offers = cluster_offers.assign(counts=cluster_offers.count(axis=1)).sort_values(['title', 'counts']).drop_duplicates('title', keep='last').drop('counts', axis=1)\n",
    "        cluster_offers['title+desc'] = (cluster_offers['title'] + ' ' + cluster_offers['description']).str.strip()\n",
    "        cluster_offers = cluster_offers[cluster_offers['title+desc'].str.split().map(len) > 5]\n",
    "        \n",
    "        gs_titles = gs_offers['title'].tolist()\n",
    "        \n",
    "        cluster_offers_ids = cluster_offers['id']\n",
    "        cluster_offers_ids = cluster_offers_ids.values\n",
    "        all_pairs_dirty = list(combinations(cluster_offers_ids, 2))\n",
    "        all_pairs = []\n",
    "        for pair in all_pairs_dirty:\n",
    "            reversed_pair = pair[::-1]\n",
    "            if list(pair) not in gs_pairs and list(reversed_pair) not in gs_pairs:\n",
    "                all_pairs.append(pair)\n",
    "        if samples > len(all_pairs):\n",
    "            cluster_sample = len(all_pairs)\n",
    "        else:\n",
    "            cluster_sample = samples\n",
    "            \n",
    "        pairs = []\n",
    "        all_pairs_copy = copy.deepcopy(all_pairs)\n",
    "\n",
    "        while True:\n",
    "            sample_amount = cluster_sample\n",
    "            counter = 0\n",
    "            bad_pairs = []\n",
    "            sampling_range = range(0, len(all_pairs))\n",
    "            if len(sampling_range) == 0:\n",
    "                break\n",
    "            sample_amount -= len(pairs)\n",
    "            assert sample_amount >= 0\n",
    "            if sample_amount == 0:\n",
    "                break\n",
    "            if len(sampling_range) < sample_amount:\n",
    "                sample_amount = len(sampling_range)\n",
    "            sample = random.sample(sampling_range, k=sample_amount)\n",
    "            pairs.extend([all_pairs[x] for x in sample])\n",
    "            for pair in pairs:\n",
    "                if pair in all_pairs:\n",
    "                    all_pairs.remove(pair)\n",
    "                if df.loc[pair[0]]['title'] in gs_titles or df.loc[pair[1]]['title'] in gs_titles:\n",
    "                    bad_pairs.append(pair)\n",
    "                    counter += 1\n",
    "            if counter > (cluster_sample / 2):\n",
    "                remove = random.sample(range(0, len(bad_pairs)), math.ceil(counter - (cluster_sample / 2)))\n",
    "                remove_pairs = [bad_pairs[x] for x in remove]\n",
    "                for pair in remove_pairs:\n",
    "                    pairs.remove(pair)\n",
    "\n",
    "            if len(pairs) > cluster_sample:\n",
    "                print('WARNING: More pos pairs sampled than should be!')\n",
    "            if len(pairs) == cluster_sample:\n",
    "                break\n",
    "            if len(pairs) == 0 and len(all_pairs) == 0:\n",
    "                sample = random.sample(range(0, len(all_pairs_copy)), k=1)\n",
    "                pairs.extend([all_pairs_copy[x] for x in sample])\n",
    "                no_clean_pair_cluster += 1\n",
    "                break\n",
    "        if len(pairs) == 0:\n",
    "            no_pair_at_all +=1\n",
    "\n",
    "        overall_pairs.extend(pairs)\n",
    "            \n",
    "    with open ('../../../data/processed/build-training-sets/'+set_label+'/'+cat+'/positives/'+cat+'_train_positive'+'_'+set_label+'.txt', \"a\") as f:\n",
    "        for item in overall_pairs:\n",
    "            f.write(str(item[0])+'#'+str(item[1])+'#'+'1'+'\\n')\n",
    "    with open('../../../data/processed/build-training-sets/'+set_label+'/'+cat+'/positives/'+cat+'_train_positive'+'_'+set_label+'.txt', \"r\") as f:\n",
    "        print(cat+ ' Positive Training samples: '+str(sum(1 for _ in f)))\n",
    "        print('For '+str(no_clean_pair_cluster)+' clusters, there was no clean training pair!')\n",
    "        print('For '+str(no_pair_at_all)+' cluster pairs, there was no training pair at all!')\n",
    "\n",
    "def get_similarities_title(df, pairs):\n",
    "    left = []\n",
    "    right = []\n",
    "    for pair in pairs:\n",
    "        left.append(pair[0])\n",
    "        right.append(pair[1])\n",
    "    titles_1 = df.loc[left]['title'].values\n",
    "    titles_2 = df.loc[right]['title'].values\n",
    "\n",
    "    titles_zipped = zip(titles_1, titles_2)\n",
    "\n",
    "    similarities = []\n",
    "\n",
    "    for titles in titles_zipped:\n",
    "        similarities.append(textdistance.Jaccard(qval=None).normalized_similarity(titles[0], titles[1]))\n",
    "    return similarities\n",
    "\n",
    "def processInput(cluster_pair):\n",
    "    df = cluster_pair[1]\n",
    "    cluster1 = df[df['cluster_id']==cluster_pair[0][0]]\n",
    "    cluster2 = df[df['cluster_id']==cluster_pair[0][1]]\n",
    "    string1 = cluster1['title'].str.cat()\n",
    "    string2 = cluster2['title'].str.cat()\n",
    "    sim = textdistance.Jaccard(qval=None).normalized_similarity(string1, string2)\n",
    "    return sim\n",
    "    \n",
    "def build_negative_pairs(df, gs, samples, sample_clusters, set_label, clu_percent=1.0):\n",
    "    cat = basename(gs).replace('_gs.json.gz', '')\n",
    "    os.makedirs(os.path.dirname('../../../data/processed/build-training-sets/'+set_label+'/'+cat+'/negatives/'), exist_ok=True)\n",
    "    try:\n",
    "        os.remove('../../../data/processed/build-training-sets/'+set_label+'/'+cat+'/negatives/'+cat+'_train_negative'+'_'+set_label+'.txt')\n",
    "    except OSError:\n",
    "        pass\n",
    "    clusters, gs_pairs, gs_offers = get_clusters_for_gs_pairs(df, gs)\n",
    "    \n",
    "    if clu_percent != 1.0:\n",
    "        sample = random.sample(range(0, len(clusters)), k=int(round(clu_percent * len(clusters))))\n",
    "        clusters_new = [clusters[x] for x in sample]\n",
    "        clusters = clusters_new\n",
    "\n",
    "    no_clean_pair_cluster = 0\n",
    "    all_cluster_pairs_already_seen = 0\n",
    "    no_pair_at_all = 0\n",
    "    overall_pairs = []\n",
    "    \n",
    "    cluster_sims = pd.DataFrame(index=clusters, columns=clusters, dtype=np.float64)\n",
    "    cluster_pairs = list(combinations(clusters, 2))\n",
    "    \n",
    "    for cluster_pair in tqdm.tqdm(cluster_pairs):\n",
    "        cluster1 = df[df['cluster_id']==cluster_pair[0]]\n",
    "        cluster2 = df[df['cluster_id']==cluster_pair[1]]\n",
    "        string1 = cluster1['title'].str.cat()\n",
    "        string2 = cluster2['title'].str.cat()\n",
    "        sim = textdistance.Jaccard(qval=None).normalized_similarity(string1, string2)\n",
    "        cluster_sims.at[cluster_pair[0], cluster_pair[1]] = sim\n",
    "        cluster_sims.at[cluster_pair[1], cluster_pair[0]] = sim\n",
    "        \n",
    "    pairs_to_consider_global = []\n",
    "    \n",
    "    sim_stats = [[] for x in range(50)]\n",
    "\n",
    "    for cluster in clusters:\n",
    "        pairs_to_consider = []\n",
    "        high_sim_clusters = cluster_sims.loc[cluster,:].nlargest(50)\n",
    "        for ix, value in enumerate(high_sim_clusters.values):\n",
    "            sim_stats[ix].append(value)\n",
    "        n_most_similar = list(high_sim_clusters[0:sample_clusters].index)\n",
    "        pairs = [[cluster, x] for x in n_most_similar]\n",
    "        for pair in pairs:\n",
    "            if pair not in pairs_to_consider_global and pair[::-1] not in pairs_to_consider_global:\n",
    "                pairs_to_consider.append(pair)\n",
    "                pairs_to_consider_global.append(pair)\n",
    "        \n",
    "        all_pairs_unnormalized = []\n",
    "        pair_list_lengths = []\n",
    "\n",
    "        if len(pairs_to_consider) == 0:\n",
    "            all_cluster_pairs_already_seen += 1\n",
    "            continue\n",
    "        \n",
    "        for cluster_pair in pairs_to_consider:\n",
    "\n",
    "            cluster_offers1 = df[df['cluster_id'] == cluster_pair[0]]\n",
    "            cluster_offers1 = cluster_offers1.dropna(subset=['title'])\n",
    "            cluster_offers1 = cluster_offers1.assign(counts=cluster_offers1.count(axis=1)).sort_values(['title', 'counts']).drop_duplicates('title', keep='last').drop('counts', axis=1)\n",
    "            cluster_offers1['title+desc'] = (cluster_offers1['title'] + ' ' + cluster_offers1['description']).str.strip()\n",
    "            cluster_offers1 = cluster_offers1[cluster_offers1['title+desc'].str.split().map(len) > 5]\n",
    "\n",
    "            cluster_offers2 = df[df['cluster_id'] == cluster_pair[1]]\n",
    "            cluster_offers2 = cluster_offers2.dropna(subset=['title'])\n",
    "            cluster_offers2 = cluster_offers2.assign(counts=cluster_offers2.count(axis=1)).sort_values(['title', 'counts']).drop_duplicates('title', keep='last').drop('counts', axis=1)\n",
    "            cluster_offers2['title+desc'] = (cluster_offers2['title'] + ' ' + cluster_offers2['description']).str.strip()\n",
    "            cluster_offers2 = cluster_offers2[cluster_offers2['title+desc'].str.split().map(len) > 5]\n",
    "\n",
    "            pair_ids_1 = cluster_offers1['id'].values\n",
    "            pair_ids_2 = cluster_offers2['id'].values\n",
    "\n",
    "            temp_all_pairs_dirty = list(product(pair_ids_1, pair_ids_2))\n",
    "            temp_all_pairs = []\n",
    "\n",
    "            for pair in temp_all_pairs_dirty:\n",
    "                reversed_pair = pair[::-1]\n",
    "                if list(pair) not in gs_pairs and list(reversed_pair) not in gs_pairs:\n",
    "                    temp_all_pairs.append(pair)\n",
    "            pair_list_lengths.append(len(temp_all_pairs))\n",
    "            all_pairs_unnormalized.append(temp_all_pairs)\n",
    "\n",
    "        min_length = min(pair_list_lengths)\n",
    "        all_pairs = []\n",
    "        for pair_list in all_pairs_unnormalized:\n",
    "            sample_min_length = random.sample(range(0, len(pair_list)), k=min_length)\n",
    "            new_pair_list = [pair_list[x] for x in sample_min_length]\n",
    "            all_pairs.extend(new_pair_list)\n",
    "\n",
    "        gs_titles = gs_offers['title'].tolist()\n",
    "\n",
    "        if samples > len(all_pairs):\n",
    "            cluster_sample = len(all_pairs)\n",
    "        else:\n",
    "            cluster_sample = samples        \n",
    "                  \n",
    "        pairs = []\n",
    "        all_pairs_copy = copy.deepcopy(all_pairs)\n",
    "\n",
    "        while True:\n",
    "            sample_amount = cluster_sample\n",
    "            counter = 0\n",
    "            bad_pairs = []\n",
    "            sampling_range = range(0, len(all_pairs))\n",
    "            if len(sampling_range) == 0:\n",
    "                break\n",
    "            sample_amount -= len(pairs)\n",
    "            assert sample_amount >= 0\n",
    "            if sample_amount == 0:\n",
    "                break\n",
    "            if len(sampling_range) < sample_amount:\n",
    "                sample_amount = len(sampling_range)\n",
    "            sample = random.sample(sampling_range, k=sample_amount)\n",
    "            pairs.extend([all_pairs[x] for x in sample])\n",
    "            for pair in pairs:\n",
    "                if pair in all_pairs:\n",
    "                    all_pairs.remove(pair)\n",
    "                if df.loc[pair[0]]['title'] in gs_titles or df.loc[pair[1]]['title'] in gs_titles:\n",
    "                    bad_pairs.append(pair)\n",
    "                    counter += 1\n",
    "            if counter > (cluster_sample / 2):\n",
    "                remove = random.sample(range(0, len(bad_pairs)), math.ceil(counter - (cluster_sample / 2)))\n",
    "                remove_pairs = [bad_pairs[x] for x in remove]\n",
    "                for pair in remove_pairs:\n",
    "                    pairs.remove(pair)\n",
    "\n",
    "            if len(pairs) > cluster_sample:\n",
    "                print('WARNING: More pos pairs sampled than should be!')\n",
    "            if len(pairs) == cluster_sample:\n",
    "                break\n",
    "            if len(pairs) == 0 and len(all_pairs) == 0:\n",
    "                sample = random.sample(range(0, len(all_pairs_copy)), k=1)\n",
    "                pairs.extend([all_pairs_copy[x] for x in sample])\n",
    "                no_clean_pair_cluster += 1\n",
    "                break\n",
    "        if len(pairs) == 0:\n",
    "            no_pair_at_all +=1\n",
    "        overall_pairs.extend(pairs)\n",
    "    \n",
    "        with open ('../../../data/processed/build-training-sets/clustersim_info_'+cat+'.csv', \"w\") as f:\n",
    "            counter = 1\n",
    "            f.write('n_most_similar_cluster'+','+'mean_sim'+','+'std_sim'+'\\n')\n",
    "            for stat in sim_stats:\n",
    "                f.write(str(counter)+','+str(np.mean(stat))+','+str(np.std(stat))+'\\n')\n",
    "                counter = counter + 1\n",
    "                \n",
    "    with open ('../../../data/processed/build-training-sets/'+set_label+'/'+cat+'/negatives/'+cat+'_train_negative'+'_'+set_label+'.txt', \"a\") as f:\n",
    "        for item in overall_pairs:\n",
    "            f.write(str(item[0])+'#'+str(item[1])+'#'+'0'+'\\n')\n",
    "            \n",
    "    with open('../../../data/processed/build-training-sets/'+set_label+'/'+cat+'/negatives/'+cat+'_train_negative'+'_'+set_label+'.txt', \"r\") as f:\n",
    "        print(cat+' Negative Training samples: '+str(sum(1 for _ in f)))\n",
    "        print('For '+str(all_cluster_pairs_already_seen)+' clusters, all 5 most similar pairs were already sampled!')\n",
    "        print('For '+str(no_clean_pair_cluster)+' cluster pairs, there was no clean training pair!')\n",
    "        print('For '+str(no_pair_at_all)+' cluster pairs, there was no training pair at all!')\n",
    "\n",
    "def combine_sets(cat, set_label):\n",
    "    pairs = []\n",
    "    with open('../../../data/processed/build-training-sets/'+set_label+'/'+cat+'/positives/'+cat+'_train_positive'+'_'+set_label+'.txt', 'r') as pf:\n",
    "        for line in pf:\n",
    "            pairs.append(line.strip())\n",
    "    with open('../../../data/processed/build-training-sets/'+set_label+'/'+cat+'/negatives/'+cat+'_train_negative'+'_'+set_label+'.txt', 'r') as nf:\n",
    "        for line in nf:\n",
    "            pairs.append(line.strip())\n",
    "    with open('../../../data/processed/build-training-sets/'+set_label+'/'+cat+'/'+cat+'_train_combined'+'_'+set_label+'.txt', 'w') as outfile:\n",
    "        for pair in pairs:\n",
    "            outfile.write(pair+'\\n')\n",
    "\n",
    "def double_check_gs_inclusion(cat, set_label):\n",
    "    train_pairs = []\n",
    "    gs_pairs = []\n",
    "    counter = 0\n",
    "    with open('../../../data/processed/build-training-sets/'+set_label+'/'+cat+'/'+cat+'_train_combined'+'_'+set_label+'.txt', 'r') as ts_f:\n",
    "        for line in ts_f:\n",
    "            line = line.strip().split('#')\n",
    "            train_pairs.append([line[0], line[1]])\n",
    "            \n",
    "    gs = pd.read_json('../../../data/raw/wdc-lspc/gold-standards/'+cat+'_gs.json.gz', lines=True)\n",
    "    gs_ids = list(gs['pair_id'].values)\n",
    "    for x in gs_ids:\n",
    "        split = x.split('#')\n",
    "        gs_pairs.append([split[0],split[1]])\n",
    "\n",
    "    for pair in gs_pairs:\n",
    "        rev = copy.deepcopy(pair).reverse()\n",
    "        if pair in train_pairs or rev in train_pairs:\n",
    "            counter += 1\n",
    "    \n",
    "    if counter == 0:\n",
    "        print('No gs pairs are in the '+cat+' training set.')\n",
    "    else:\n",
    "        print('WARNING: Number of gs pairs in the '+cat+' training set: '+str(counter))\n",
    "        \n",
    "def get_ts_stats(cats, set_label, df=None):\n",
    "    print(f'Training set: {set_label}')\n",
    "    try:\n",
    "        os.remove('../../../data/processed/build-training-sets/'+set_label+'/stats'+'_'+set_label+'.txt')\n",
    "    except OSError:\n",
    "        pass\n",
    "    \n",
    "    with open('../../../data/processed/build-training-sets/'+set_label+'/stats'+'_'+set_label+'.txt', 'a') as outfile:\n",
    "            outfile.write('category,positives,negatives,combined\\n')\n",
    "    for cat in cats:   \n",
    "        pos = []\n",
    "        neg = []\n",
    "        with open('../../../data/processed/build-training-sets/'+set_label+'/'+cat+'/positives/'+cat+'_train_positive'+'_'+set_label+'.txt', 'r') as pf:\n",
    "            for line in pf:\n",
    "                pos.append(line.strip())\n",
    "        with open('../../../data/processed/build-training-sets/'+set_label+'/'+cat+'/negatives/'+cat+'_train_negative'+'_'+set_label+'.txt', 'r') as nf:\n",
    "            for line in nf:\n",
    "                neg.append(line.strip())\n",
    "        with open('../../../data/processed/build-training-sets/'+set_label+'/stats'+'_'+set_label+'.txt', 'a') as outfile:\n",
    "            outfile.write(cat+','+str(len(pos))+','+str(len(neg))+','+str(len(pos)+len(neg))+'\\n')\n",
    "        if isinstance(df, pd.DataFrame):\n",
    "            pairs = []\n",
    "            counter = 0\n",
    "            for line in neg:\n",
    "                pairs.append(line.split('#')[0:2])\n",
    "            for pair in pairs:\n",
    "                if df.loc[int(pair[0])]['title'] == df.loc[int(pair[1])]['title']:\n",
    "                    counter += 1\n",
    "        print('For '+cat+' there were '+str(counter)+ ' negatives with exact same title.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clusters in computers 745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/277140 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computers Positive Training samples: 722\n",
      "For 148 clusters, there was no clean training pair!\n",
      "For 23 cluster pairs, there was no training pair at all!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 277140/277140 [3:28:25<00:00, 22.16it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computers Negative Training samples: 2107\n",
      "For 11 clusters, all 5 most similar pairs were already sampled!\n",
      "For 0 cluster pairs, there was no clean training pair!\n",
      "For 0 cluster pairs, there was no training pair at all!\n",
      "No gs pairs are in the computers training set.\n",
      "Clusters in cameras 563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/158203 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cameras Positive Training samples: 486\n",
      "For 133 clusters, there was no clean training pair!\n",
      "For 77 cluster pairs, there was no training pair at all!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 158203/158203 [1:53:43<00:00, 23.18it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cameras Negative Training samples: 1421\n",
      "For 6 clusters, all 5 most similar pairs were already sampled!\n",
      "For 1 cluster pairs, there was no clean training pair!\n",
      "For 7 cluster pairs, there was no training pair at all!\n",
      "No gs pairs are in the cameras training set.\n",
      "Clusters in watches 617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/190036 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "watches Positive Training samples: 580\n",
      "For 118 clusters, there was no clean training pair!\n",
      "For 37 cluster pairs, there was no training pair at all!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 190036/190036 [2:11:02<00:00, 24.17it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "watches Negative Training samples: 1671\n",
      "For 11 clusters, all 5 most similar pairs were already sampled!\n",
      "For 0 cluster pairs, there was no clean training pair!\n",
      "For 3 cluster pairs, there was no training pair at all!\n",
      "No gs pairs are in the watches training set.\n",
      "Clusters in shoes 563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/158203 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shoes Positive Training samples: 530\n",
      "For 131 clusters, there was no clean training pair!\n",
      "For 33 cluster pairs, there was no training pair at all!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 158203/158203 [1:50:32<00:00, 23.85it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shoes Negative Training samples: 1529\n",
      "For 5 clusters, all 5 most similar pairs were already sampled!\n",
      "For 0 cluster pairs, there was no clean training pair!\n",
      "For 1 cluster pairs, there was no training pair at all!\n",
      "No gs pairs are in the shoes training set.\n",
      "Clusters in computers 745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/277140 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computers Positive Training samples: 1762\n",
      "For 97 clusters, there was no clean training pair!\n",
      "For 23 cluster pairs, there was no training pair at all!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 277140/277140 [3:14:10<00:00, 23.79it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computers Negative Training samples: 6321\n",
      "For 11 clusters, all 5 most similar pairs were already sampled!\n",
      "For 0 cluster pairs, there was no clean training pair!\n",
      "For 0 cluster pairs, there was no training pair at all!\n",
      "No gs pairs are in the computers training set.\n",
      "Clusters in cameras 563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/158203 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cameras Positive Training samples: 1108\n",
      "For 65 clusters, there was no clean training pair!\n",
      "For 77 cluster pairs, there was no training pair at all!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 158203/158203 [1:47:43<00:00, 24.48it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cameras Negative Training samples: 4198\n",
      "For 6 clusters, all 5 most similar pairs were already sampled!\n",
      "For 1 cluster pairs, there was no clean training pair!\n",
      "For 7 cluster pairs, there was no training pair at all!\n",
      "No gs pairs are in the cameras training set.\n",
      "Clusters in watches 617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/190036 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "watches Positive Training samples: 1418\n",
      "For 64 clusters, there was no clean training pair!\n",
      "For 37 cluster pairs, there was no training pair at all!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 190036/190036 [2:11:49<00:00, 24.02it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "watches Negative Training samples: 4979\n",
      "For 11 clusters, all 5 most similar pairs were already sampled!\n",
      "For 0 cluster pairs, there was no clean training pair!\n",
      "For 3 cluster pairs, there was no training pair at all!\n",
      "No gs pairs are in the watches training set.\n",
      "Clusters in shoes 563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/158203 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shoes Positive Training samples: 1214\n",
      "For 44 clusters, there was no clean training pair!\n",
      "For 33 cluster pairs, there was no training pair at all!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 158203/158203 [1:47:44<00:00, 24.47it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shoes Negative Training samples: 4575\n",
      "For 5 clusters, all 5 most similar pairs were already sampled!\n",
      "For 0 cluster pairs, there was no clean training pair!\n",
      "For 1 cluster pairs, there was no training pair at all!\n",
      "No gs pairs are in the shoes training set.\n",
      "Clusters in computers 745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/277140 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computers Positive Training samples: 6146\n",
      "For 97 clusters, there was no clean training pair!\n",
      "For 23 cluster pairs, there was no training pair at all!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 38737/277140 [26:03<3:40:12, 18.04it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|██████████| 277140/277140 [3:08:53<00:00, 24.45it/s]  \n"
     ]
    }
   ],
   "source": [
    "cats = ['computers', 'cameras', 'watches', 'shoes']\n",
    "\n",
    "training_sets = ['1-3-10-1.0', '3-9-10-1.0', '15-45-10-1.0', '50-150-10-1.0']\n",
    "\n",
    "for train in training_sets:\n",
    "    params = [int(float(x)) for x in train.split('-')]\n",
    "    \n",
    "    for cat in cats:\n",
    "        build_positive_pairs(data_df, '../../../data/raw/wdc-lspc/gold-standards/'+cat+'_gs.json.gz', params[0], train)\n",
    "        build_negative_pairs(data_df, '../../../data/raw/wdc-lspc/gold-standards/'+cat+'_gs.json.gz', params[1], params[2], train)\n",
    "        combine_sets(cat, train)\n",
    "        double_check_gs_inclusion(cat, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = ['computers', 'cameras', 'watches', 'shoes']\n",
    "\n",
    "training_sets = ['1-3-10-1.0', '3-9-10-1.0', '15-45-10-1.0', '50-150-10-1.0']\n",
    "\n",
    "for train in training_sets:\n",
    "    get_ts_stats(cats, train, data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
