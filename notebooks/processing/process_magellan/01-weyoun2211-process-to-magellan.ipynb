{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare data for magellan and deepmatcher input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Load the \"autoreload\" extension so that code can change\n",
    "%load_ext autoreload\n",
    "\n",
    "# OPTIONAL: always reload modules so that as you change code in src, it gets loaded\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import py_entitymatching as em\n",
    "import sys\n",
    "sys.path.append('../../../')\n",
    "\n",
    "from src.data import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_magellan(file, columns_to_preprocess, experiment_name, validation_set=None):\n",
    "    \n",
    "    columns_preprocess_magellan = ['ltable_'+col for col in columns_to_preprocess]\n",
    "    columns_preprocess_magellan.extend(['rtable_'+col for col in columns_to_preprocess])\n",
    "    \n",
    "    data_df = pd.read_json(file, compression='gzip', lines=True)\n",
    "    data_df.fillna(value=pd.np.nan, inplace=True)\n",
    "    data_df.replace(pd.np.nan, '', inplace=True)\n",
    "    \n",
    "    # change column naming to magellan format\n",
    "    cols = list(data_df.columns)\n",
    "    for i, col in enumerate(cols):\n",
    "        if '_left' in col:\n",
    "            col = col.replace('_left','')\n",
    "            cols[i] = 'ltable_'+col\n",
    "        if '_right' in col:\n",
    "            col = col.replace('_right','')\n",
    "            cols[i] = 'rtable_'+col\n",
    "    data_df.columns = cols\n",
    "    # preprocess selected columns\n",
    "    for column in columns_preprocess_magellan:\n",
    "        data_df[column] = utils.preprocess_string_column(data_df[column])\n",
    "    \n",
    "    # build left and right subsets\n",
    "    left_df = data_df[[col for col in data_df.columns if 'ltable_' in col]].copy()\n",
    "    left_df.drop_duplicates(subset='ltable_id', inplace=True)\n",
    "    right_df = data_df[[col for col in data_df.columns if 'rtable_' in col]].copy()\n",
    "    right_df.drop_duplicates(subset='rtable_id',inplace=True)\n",
    "    \n",
    "    # assign magellan ids in subsets\n",
    "    left_df['mag_id'] = range(0, len(left_df))\n",
    "    right_df['mag_id'] = range(0, len(right_df))\n",
    "    \n",
    "    # use magellan ids and assign global pair id\n",
    "    len_assert = len(data_df)\n",
    "    data_df = data_df.merge(left_df[['ltable_id', 'mag_id']], how='left', on='ltable_id')\n",
    "    data_df.rename(columns={'mag_id': 'ltable_mag_id'}, inplace=True)\n",
    "    data_df = data_df.merge(right_df[['rtable_id', 'mag_id']], how='left', on='rtable_id')\n",
    "    data_df.rename(columns={'mag_id': 'rtable_mag_id'}, inplace=True)\n",
    "    data_df['_id'] = range(0, len(data_df))\n",
    "    assert len(data_df) == len_assert\n",
    "    \n",
    "    left_df.drop(columns='ltable_id', inplace=True)\n",
    "    right_df.drop(columns='rtable_id', inplace=True)\n",
    "    \n",
    "    left_cols = left_df.columns\n",
    "    left_df.columns = [col.replace('ltable_','') for col in left_cols]\n",
    "    \n",
    "    right_cols = right_df.columns\n",
    "    right_df.columns = [col.replace('rtable_','') for col in right_cols]\n",
    "    \n",
    "    file_name = os.path.basename(file)\n",
    "    new_file_name = file_name.replace('.json.gz', '_magellan_')\n",
    "    \n",
    "    out_path1 = '../../../data/processed/wdc-lspc/magellan/{}/'.format(experiment_name)\n",
    "    out_path2 = '../../../data/processed/wdc-lspc/magellan/{}/formatted/'.format(experiment_name)\n",
    "    \n",
    "    os.makedirs(out_path2, exist_ok=True)\n",
    "\n",
    "    left_df.to_csv(out_path1+new_file_name+'left.csv.gz', compression='gzip', header=True, index=False)\n",
    "    right_df.to_csv(out_path1+new_file_name+'right.csv.gz', compression='gzip', header=True, index=False)\n",
    "    data_df.to_csv(out_path1+new_file_name+'pairs.csv.gz', compression='gzip', header=True, index=False)\n",
    "    \n",
    "    \n",
    "    # magellan formatting for py_entitymatching\n",
    "    A = em.read_csv_metadata(out_path1+new_file_name+'left.csv.gz', key='mag_id')\n",
    "    em.to_csv_metadata(A, out_path2+new_file_name+'left_formatted.csv')\n",
    "    B = em.read_csv_metadata(out_path1+new_file_name+'right.csv.gz', key='mag_id')\n",
    "    em.to_csv_metadata(B, out_path2+new_file_name+'right_formatted.csv')\n",
    "\n",
    "    \n",
    "    \n",
    "    C = em.read_csv_metadata(out_path1+new_file_name+'pairs.csv.gz', \n",
    "                     key='_id',\n",
    "                     ltable=A, rtable=B, \n",
    "                     fk_ltable='ltable_mag_id', fk_rtable='rtable_mag_id')\n",
    "    \n",
    "    if isinstance(validation_set,type(None)):\n",
    "        \n",
    "        em.to_csv_metadata(C, out_path2+new_file_name+'pairs_formatted.csv')\n",
    "        \n",
    "    else:\n",
    "        validation_ids_df = pd.read_csv(validation_set)\n",
    "        \n",
    "        validation_df = C[C['pair_id'].isin(validation_ids_df['pair_id'].values)]\n",
    "        train_df = C[~C['pair_id'].isin(validation_ids_df['pair_id'].values)]\n",
    "        \n",
    "        em.to_csv_metadata(C, out_path2+new_file_name+'pairs_formatted.csv')\n",
    "        \n",
    "        new_file_name = new_file_name.replace('train','trainonly')\n",
    "        \n",
    "        em.to_csv_metadata(train_df, out_path2+new_file_name+'pairs_formatted.csv')\n",
    "        \n",
    "        valid_name = new_file_name.replace('trainonly','valid')\n",
    "    \n",
    "        em.to_csv_metadata(validation_df, out_path2+valid_name+'pairs_formatted.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_preprocess = ['title', 'description', 'brand', 'specTableContent']\n",
    "\n",
    "# learning-curve experiment\n",
    "for file in glob.glob('../../../data/raw/wdc-lspc/training-sets/*'):\n",
    "    valid = file.replace('training', 'validation')\n",
    "    valid = valid.replace('train', 'valid')\n",
    "    valid = valid.replace('.json.gz', '.csv')\n",
    "    preprocess_magellan(file, columns_to_preprocess, experiment_name='learning-curve', validation_set=valid)\n",
    "\n",
    "for file in glob.glob('../../../data/raw/wdc-lspc/gold-standards/*'):    \n",
    "    preprocess_magellan(file, columns_to_preprocess,experiment_name='learning-curve')\n",
    "\n",
    "    \n",
    "# noisy training data experiment\n",
    "for file in glob.glob('../../../data/interim/wdc-lspc/training-sets-noised/*'):\n",
    "    preprocess_magellan(file, columns_to_preprocess, experiment_name='training-sets-noised', validation_set='../../../data/raw/wdc-lspc/validation-sets/computers_valid_xlarge.csv')\n",
    "    \n",
    "for file in glob.glob('../../../data/raw/wdc-lspc/gold-standards/*'):\n",
    "    preprocess_magellan(file, columns_to_preprocess,experiment_name='training-sets-noised')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
