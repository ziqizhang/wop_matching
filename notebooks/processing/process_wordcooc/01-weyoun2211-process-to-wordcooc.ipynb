{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare data for word cooccurrence input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Load the \"autoreload\" extension so that code can change\n",
    "%load_ext autoreload\n",
    "\n",
    "# OPTIONAL: always reload modules so that as you change code in src, it gets loaded\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import functools\n",
    "import json\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../../')\n",
    "from src.data import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_df_columns_to_wordocc(file, columns_preprocess_wordcooc, feature_combinations):\n",
    "    data_df = pd.read_json(file, compression='gzip', lines=True)\n",
    "    data_df.fillna(value=pd.np.nan, inplace=True)\n",
    "    data_df.replace(pd.np.nan, '', inplace=True)\n",
    "    \n",
    "    # preprocess selected columns\n",
    "    for column in columns_preprocess_wordcooc:\n",
    "        data_df[column] = utils.preprocess_string_column(data_df[column])\n",
    "    \n",
    "    # build combined features for every feature combination\n",
    "    for feature_combination in feature_combinations:\n",
    "        feats_to_combine = feature_combination.split('+')\n",
    "        data_df[feature_combination+'_wordocc_left'] = data_df[feats_to_combine[0]+'_left']\n",
    "        data_df[feature_combination+'_wordocc_right'] = data_df[feats_to_combine[0]+'_right']\n",
    "        \n",
    "        for feat_to_combine in feats_to_combine[1:]:\n",
    "            data_df[feature_combination+'_wordocc_left'] += (' '+data_df[feat_to_combine+'_left'])\n",
    "            data_df[feature_combination+'_wordocc_right'] += (' '+data_df[feat_to_combine+'_right'])\n",
    "        \n",
    "        data_df[feature_combination+'_wordocc_left'] = data_df[feature_combination+'_wordocc_left'].str.strip()\n",
    "        data_df[feature_combination+'_wordocc_right'] = data_df[feature_combination+'_wordocc_right'].str.strip()\n",
    "        \n",
    "    return data_df\n",
    "\n",
    "def transform_columns_to_wordcount(data_df, feature_combinations, test_df):\n",
    "    \n",
    "    words = {}\n",
    "    \n",
    "    for feature_combination in feature_combinations:\n",
    "        \n",
    "        # build relevant strings for vocabulary\n",
    "        all_left_strings = data_df[['id_left',feature_combination+'_wordocc_left']].copy()\n",
    "        all_left_strings = all_left_strings.rename(columns={'id_left':'id',feature_combination+'_wordocc_left':feature_combination})\n",
    "        all_right_strings = data_df[['id_right',feature_combination+'_wordocc_right']].copy()\n",
    "        all_right_strings = all_right_strings.rename(columns={'id_right':'id',feature_combination+'_wordocc_right':feature_combination})\n",
    "        all_unique_strings = pd.concat([all_left_strings,all_right_strings])\n",
    "        all_unique_strings = all_unique_strings.drop_duplicates(subset='id')\n",
    "        \n",
    "        # learn vocabulary\n",
    "        count_vectorizer = CountVectorizer(min_df=2, binary=True)\n",
    "        count_vectorizer.fit(all_unique_strings[feature_combination])\n",
    "        \n",
    "        words[feature_combination] = count_vectorizer.get_feature_names()\n",
    "        \n",
    "        # apply binary word occurrence\n",
    "        left_matrix = count_vectorizer.transform(data_df[feature_combination+'_wordocc_left'])\n",
    "        right_matrix = count_vectorizer.transform(data_df[feature_combination+'_wordocc_right'])\n",
    "        data_df[feature_combination+'_wordocc_left'] = [x for x in left_matrix]\n",
    "        data_df[feature_combination+'_wordocc_right'] = [x for x in right_matrix]\n",
    "        \n",
    "        if not isinstance(test_df,type(None)):\n",
    "            left_matrix_test = count_vectorizer.transform(test_df[feature_combination+'_wordocc_left'])\n",
    "            right_matrix_test = count_vectorizer.transform(test_df[feature_combination+'_wordocc_right'])\n",
    "            test_df[feature_combination+'_wordocc_left'] = [x for x in left_matrix_test]\n",
    "            test_df[feature_combination+'_wordocc_right'] = [x for x in right_matrix_test]\n",
    "            \n",
    "    return data_df, test_df, words\n",
    "        \n",
    "def transform_columns_to_wordcooc(data_df, feature_combinations, test_df):\n",
    "    \n",
    "    for feature_combination in feature_combinations:\n",
    "        data_df[feature_combination+'_wordcooc'] = list(map(lambda x, y: x.multiply(y).astype(int), data_df[feature_combination+'_wordocc_left'].values, data_df[feature_combination+'_wordocc_right'].values))\n",
    "\n",
    "        if not isinstance(test_df,type(None)):\n",
    "            test_df[feature_combination+'_wordcooc'] = list(map(lambda x, y: x.multiply(y).astype(int), test_df[feature_combination+'_wordocc_left'].values, test_df[feature_combination+'_wordocc_right'].values))\n",
    "            \n",
    "    return data_df, test_df\n",
    "            \n",
    "def preprocess_wordcooc(file, columns_to_preprocess, feature_combinations, experiment_name, valid_set=None, test_set=None):\n",
    "    \n",
    "    columns_preprocess_wordcooc = [col+'_left' for col in columns_to_preprocess]\n",
    "    columns_preprocess_wordcooc.extend([col+'_right' for col in columns_to_preprocess])\n",
    "    \n",
    "    main_df = process_df_columns_to_wordocc(file, columns_preprocess_wordcooc, feature_combinations)\n",
    "    \n",
    "    if not isinstance(test_set,type(None)):\n",
    "        test_df = process_df_columns_to_wordocc(test_set, columns_preprocess_wordcooc, feature_combinations)\n",
    "    else:\n",
    "        test_df = None\n",
    "    \n",
    "    main_df, test_df, words = transform_columns_to_wordcount(main_df, feature_combinations, test_df)\n",
    "    main_df, test_df = transform_columns_to_wordcooc(main_df, feature_combinations, test_df)\n",
    "    \n",
    "    main_name = os.path.basename(file)\n",
    "    new_main_name = main_name.replace('.json.gz', '_wordcooc')\n",
    "        \n",
    "    out_path = '../../../data/processed/wdc-lspc/wordcooc/{}/'.format(experiment_name)\n",
    "    \n",
    "    os.makedirs(out_path+'feature-names/', exist_ok=True)\n",
    "    \n",
    "    with open(out_path+'feature-names/'+new_main_name+'_words.json', 'w') as f:\n",
    "            json.dump(words, f, ensure_ascii=False)\n",
    "    \n",
    "    if isinstance(valid_set,type(None)):\n",
    "        main_df.to_pickle(out_path+new_main_name+'.pkl.gz', compression='gzip')\n",
    "    else:\n",
    "        validation_ids_df = pd.read_csv(valid_set)\n",
    "        validation_df = main_df[main_df['pair_id'].isin(validation_ids_df['pair_id'].values)]  \n",
    "        \n",
    "        main_df.to_pickle(out_path+new_main_name+'.pkl.gz', compression='gzip')\n",
    "        valid_name = new_main_name.replace('train','valid')\n",
    "        validation_df.to_pickle(out_path+valid_name+'.pkl.gz', compression='gzip')\n",
    "        \n",
    "    if not isinstance(test_df,type(None)):\n",
    "        test_name = os.path.basename(test_set)\n",
    "        test_name = test_name.replace('.json.gz','')\n",
    "        new_test_name = new_main_name+'_'+test_name\n",
    "        \n",
    "        test_df.to_pickle(out_path+new_test_name+'.pkl.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_preprocess = ['title', 'description', 'brand', 'specTableContent']\n",
    "feature_combinations = ['title','title+description','title+description+brand','title+description+brand+specTableContent']\n",
    "    \n",
    "# learning-curve experiment\n",
    "for file in glob.glob('../../../data/raw/wdc-lspc/training-sets/*'):\n",
    "    \n",
    "    valid = file.replace('training', 'validation')\n",
    "    valid = valid.replace('train', 'valid')\n",
    "    valid = valid.replace('.json.gz', '.csv')\n",
    "    \n",
    "    test_cat = os.path.basename(file).split('_')[0]\n",
    "    test ='../../../data/raw/wdc-lspc/gold-standards/{}_gs.json.gz'.format(test_cat)\n",
    "    \n",
    "    preprocess_wordcooc(file, columns_to_preprocess, feature_combinations, experiment_name='learning-curve', valid_set=valid, test_set=test)\n",
    "    \n",
    "    \n",
    "# noisy training data experiment\n",
    "for file in glob.glob('../../../data/interim/wdc-lspc/training-sets-noised/*'):\n",
    "    preprocess_wordcooc(file, columns_to_preprocess, feature_combinations, experiment_name='training-sets-noised', valid_set='../../../data/raw/wdc-lspc/validation-sets/computers_valid_xlarge.csv', test_set='../../../data/raw/wdc-lspc/gold-standards/computers_gs.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
