{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run word cooccurrence experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Load the \"autoreload\" extension so that code can change\n",
    "%load_ext autoreload\n",
    "\n",
    "# OPTIONAL: always reload modules so that as you change code in src, it gets loaded\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "import scipy\n",
    "\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "import json\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_wordcooc(train_set, valid_set, test_set, feature_combinations, classifiers, experiment_name, write_test_set_for_inspection=False):\n",
    "    \n",
    "    train_path = os.path.dirname(train_set)\n",
    "    train_file = os.path.basename(train_set)\n",
    "    test_path = os.path.dirname(test_set)\n",
    "    test_file =os.path.basename(test_set)\n",
    "    report_train_name = train_file.replace('.pkl.gz','')\n",
    "    report_test_name = test_file.replace('.pkl.gz','')\n",
    "    \n",
    "    os.makedirs(os.path.dirname('../../../reports/wordcooc/{}/'.format(experiment_name)),\n",
    "                    exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        os.remove('../../../reports/wordcooc/{}/{}_{}.csv'.format(experiment_name, report_train_name, report_test_name))\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "    with open('../../../reports/wordcooc/{}/{}_{}.csv'.format(experiment_name, report_train_name, report_test_name),\"w\") as f:\n",
    "        f.write('feature#####model#####mean_train_score#####std_train_score#####mean_valid_score#####std_valid_score#####precision_test#####recall_test#####f1_test#####best_params#####train_time#####prediction_time#####feature_importance#####experiment_name#####train_set#####test_set\\n')\n",
    "\n",
    "    for feature_combination in feature_combinations:\n",
    "    \n",
    "        train_original_df = pd.read_pickle(train_set, compression='gzip')\n",
    "        gs_df = pd.read_pickle(test_set, compression='gzip')\n",
    "        \n",
    "        feature_file_name = train_file.replace('.pkl.gz','_words.json')\n",
    "        \n",
    "        with open(train_path+'/feature-names/'+feature_file_name) as json_data:\n",
    "            words = json.load(json_data)\n",
    "        \n",
    "        validation_ids_df = pd.read_pickle(valid_set, compression='gzip')\n",
    "        val_df = train_original_df[train_original_df['pair_id'].isin(validation_ids_df['pair_id'].values)]\n",
    "        train_only_df = train_original_df[~train_original_df['pair_id'].isin(validation_ids_df['pair_id'].values)]\n",
    "        train_only_df = train_only_df.sample(frac=1, random_state=42)\n",
    "        \n",
    "        pos_neg = train_original_df['label'].value_counts()\n",
    "        pos_neg = round(pos_neg[0] / pos_neg[1])\n",
    "        \n",
    "        train_ind = []\n",
    "        val_ind = []\n",
    "        \n",
    "        for i in range(len(train_only_df)-1):\n",
    "            train_ind.append(-1)\n",
    "\n",
    "        for i in range(len(val_df)-1):\n",
    "            val_ind.append(0)       \n",
    "        \n",
    "        ps = PredefinedSplit(test_fold=np.concatenate((train_ind,val_ind)))\n",
    "\n",
    "        train_df = pd.concat([train_only_df,val_df])\n",
    "        \n",
    "        for k, v in classifiers.items():\n",
    "\n",
    "            classifier = v['clf']\n",
    "\n",
    "            # add pos_neg ratio to XGBoost params\n",
    "            if k == 'XGBoost':\n",
    "                v['params']['scale_pos_weight']: [1, pos_neg]\n",
    "\n",
    "            model = RandomizedSearchCV(cv=ps, estimator=classifier, param_distributions=v['params'], random_state=42, n_jobs=4, scoring='f1', n_iter=500, pre_dispatch=8, return_train_score=True)\n",
    "\n",
    "            feats_train = scipy.sparse.vstack(train_df[feature_combination+'_wordcooc'])\n",
    "            labels_train = train_df['label']\n",
    "            feats_gs = scipy.sparse.vstack(gs_df[feature_combination+'_wordcooc'])\n",
    "            labels_gs = gs_df['label']           \n",
    "            \n",
    "            model.fit(feats_train, labels_train)\n",
    "\n",
    "            parameters = model.best_params_\n",
    "\n",
    "            score_names = ['mean_train_score', 'std_train_score', 'mean_test_score', 'std_test_score']\n",
    "            scores = {}\n",
    "            score_string = ''\n",
    "            for name in score_names:\n",
    "                scores[name] = model.cv_results_[name][model.best_index_]\n",
    "                score_string = score_string + name + ': ' + str(scores[name]) + ' '\n",
    "\n",
    "            if k == 'LogisticRegression'or k == 'LinearSVC':\n",
    "                most_important_features = model.best_estimator_.coef_\n",
    "                word_importance = zip(words[feature_combination], most_important_features[0].tolist())\n",
    "                word_importance = sorted(word_importance, key=lambda importance: importance[1], reverse=True)\n",
    "            if k == 'RandomForest' or k == 'DecisionTree':\n",
    "                most_important_features = model.best_estimator_.feature_importances_\n",
    "                word_importance = zip(words[feature_combination], most_important_features.tolist())\n",
    "                word_importance =sorted(word_importance, key=lambda importance: importance[1], reverse=True)\n",
    "            if k == 'NaiveBayes':\n",
    "                word_importance = ''\n",
    "            if k == 'XGBoost':\n",
    "                most_important_features = model.best_estimator_.feature_importances_\n",
    "                word_importance = zip(words[feature_combination], most_important_features.tolist())\n",
    "                word_importance =sorted(word_importance, key=lambda importance: importance[1], reverse=True)\n",
    "\n",
    "\n",
    "            if k == 'LogisticRegression':\n",
    "                learner = LogisticRegression(random_state=42, solver='liblinear', **parameters)\n",
    "            elif k == 'NaiveBayes':\n",
    "                learner = BernoulliNB()\n",
    "            elif k == 'DecisionTree':\n",
    "                learner = DecisionTreeClassifier(random_state=42, **parameters)\n",
    "            elif k == 'LinearSVC':\n",
    "                learner = LinearSVC(random_state=42, dual=False, **parameters)\n",
    "            elif k == 'RandomForest':\n",
    "                learner = RandomForestClassifier(random_state=42, n_jobs=4, **parameters)\n",
    "            elif k == 'XGBoost':\n",
    "                learner = xgb.XGBClassifier(random_state=42, n_jobs=4, **parameters)\n",
    "            else:\n",
    "                print('Learner is not a valid option')\n",
    "                break\n",
    "\n",
    "\n",
    "            model = learner\n",
    "            feats_train = scipy.sparse.vstack(train_only_df[feature_combination+'_wordcooc'])\n",
    "            labels_train = train_only_df['label']\n",
    "\n",
    "            start = time.time()\n",
    "            model.fit(feats_train, labels_train)\n",
    "            end = time.time()\n",
    "\n",
    "            train_time = end-start\n",
    "\n",
    "            start = time.time()\n",
    "            preds_gs = model.predict(feats_gs)\n",
    "            \n",
    "            end = time.time()\n",
    "\n",
    "            pred_time = end-start           \n",
    "                                \n",
    "            gs_report = classification_report(labels_gs, preds_gs, output_dict=True)\n",
    "            \n",
    "            if write_test_set_for_inspection:\n",
    "                \n",
    "                out_path = '../../../data/processed/wdc-lspc/inspection/{}/wordcooc/'.format(experiment_name)\n",
    "                os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "                \n",
    "                file_name = '_'.join([os.path.basename(train_set),os.path.basename(test_set),k,feature_combination])\n",
    "                file_name = file_name.replace('.csv', '')\n",
    "                file_name += '.csv.gz'\n",
    "                \n",
    "                test_inspection_df = gs_df.copy()\n",
    "                if k == 'LinearSVC':\n",
    "                    proba_gs = model.decision_function(feats_gs).tolist()\n",
    "                else:\n",
    "                    proba_gs = model.predict_proba(feats_gs).tolist()\n",
    "                test_inspection_df['pred'] = preds_gs\n",
    "                test_inspection_df['Class Prob'] = proba_gs\n",
    "                test_inspection_df.to_csv(out_path+file_name, compression='gzip', header=True, index=False)\n",
    "            \n",
    "            with open ('../../../reports/wordcooc/{}/{}_{}.csv'.format(experiment_name, report_train_name, report_test_name), \"a\") as f:\n",
    "                    f.write(feature_combination + '#####' + k + '#####' + str(\n",
    "                        scores['mean_train_score']) + '#####' + str(scores['std_train_score'])\n",
    "                            + '#####' + str(scores['mean_test_score']) + '#####' + str(\n",
    "                        scores['std_test_score']) + '#####' + str(gs_report['1']['precision']) + '#####' + str(\n",
    "                        gs_report['1']['recall']) + '#####' + str(gs_report['1']['f1-score'])\n",
    "                            + '#####' + str(parameters) + '#####' + str(train_time) + '#####' + str(pred_time)\n",
    "                            + '#####' + str(word_importance[0:100]) + '#####' + experiment_name + '#####'+ report_train_name+ '#####'+ report_test_name +'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {'NaiveBayes':  {'clf':BernoulliNB(),\n",
    "                            'params':{}},\n",
    "                   'XGBoost': {'clf':xgb.XGBClassifier(random_state=42, n_jobs=4), \n",
    "                                'params':{\"learning_rate\": [0.1, 0.01, 0.001],\n",
    "                           \"gamma\" : [0.01, 0.1, 0.3, 0.5, 1, 1.5, 2],\n",
    "                           \"max_depth\": [2, 4, 7, 10],\n",
    "                           \"colsample_bytree\": [0.3, 0.6, 0.8, 1.0],\n",
    "                           \"subsample\": [0.2, 0.4, 0.5, 0.6, 0.7],\n",
    "                           \"reg_alpha\": [0, 0.5, 1],\n",
    "                           \"reg_lambda\": [1, 1.5, 2, 3, 4.5],\n",
    "                           \"min_child_weight\": [1, 3, 5, 7],\n",
    "                           \"n_estimators\": [100]}},\n",
    "                   'RandomForest':  {'clf':RandomForestClassifier(random_state=42, n_jobs=4), \n",
    "                                'params':{'n_estimators': [100],\n",
    "                                 'max_features': ['sqrt', 'log2', None],\n",
    "                                 'max_depth': [2,4,7,10],\n",
    "                                 'min_samples_split': [2, 5, 10, 20],\n",
    "                                 'min_samples_leaf': [1, 2, 4, 8],\n",
    "                                 'class_weight':[None, 'balanced_subsample']\n",
    "                                 }},\n",
    "                   'DecisionTree':  {'clf':DecisionTreeClassifier(random_state=42), \n",
    "                                'params':{'max_features': ['sqrt', 'log2', None],\n",
    "                                 'max_depth': [2,4,7,10],\n",
    "                                 'min_samples_split': [2, 5, 10, 20],\n",
    "                                 'min_samples_leaf': [1, 2, 4, 8],\n",
    "                                 'class_weight':[None, 'balanced']\n",
    "                                 }},\n",
    "                   'LinearSVC':  {'clf':LinearSVC(random_state=42, dual=False),\n",
    "                      'params':{'C': [0.0001 ,0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "                      'class_weight':[None, 'balanced']}},\n",
    "                   'LogisticRegression': {'clf':LogisticRegression(random_state=42, solver='liblinear'),\n",
    "                        'params':{'C': [0.0001 ,0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "                        'class_weight':[None, 'balanced']}},\n",
    "                   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rpeeters/anaconda3/envs/magellan/lib/python3.7/site-packages/sklearn/model_selection/_search.py:266: UserWarning: The total space of parameters 1 is smaller than n_iter=500. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n",
      "/home/rpeeters/anaconda3/envs/magellan/lib/python3.7/site-packages/sklearn/model_selection/_search.py:266: UserWarning: The total space of parameters 384 is smaller than n_iter=500. Running 384 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n",
      "/home/rpeeters/anaconda3/envs/magellan/lib/python3.7/site-packages/sklearn/model_selection/_search.py:266: UserWarning: The total space of parameters 384 is smaller than n_iter=500. Running 384 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n",
      "/home/rpeeters/anaconda3/envs/magellan/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/rpeeters/anaconda3/envs/magellan/lib/python3.7/site-packages/sklearn/model_selection/_search.py:266: UserWarning: The total space of parameters 16 is smaller than n_iter=500. Running 16 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# learning-curve experiment        \n",
    "feature_combinations = ['title','title+description','title+description+brand','title+description+brand+specTableContent']\n",
    "experiment_name = 'learning-curve'\n",
    "\n",
    "for file in glob.glob('../../../data/processed/wdc-lspc/wordcooc/learning-curve/*'):\n",
    "    if 'train_' in file and '_gs' not in file:\n",
    "        valid = file.replace('train_', 'valid_')\n",
    "        \n",
    "        test_cat = os.path.basename(file).split('_')[0]\n",
    "        test = os.path.basename(file)\n",
    "        test = test.replace('.pkl.gz','_{}_gs.pkl.gz'.format(test_cat))\n",
    "        test ='../../../data/processed/wdc-lspc/wordcooc/learning-curve/{}'.format(test)\n",
    "        \n",
    "        run_wordcooc(file, valid, test, feature_combinations, classifiers, experiment_name, write_test_set_for_inspection=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noisy training data experiment\n",
    "feature_combinations = ['title+description+brand']\n",
    "experiment_name = 'training-sets-noised'\n",
    "\n",
    "for file in glob.glob('../../../data/processed/wdc-lspc/wordcooc/training-sets-noised/*'):\n",
    "    if 'train_' in file and '_gs' not in file:\n",
    "        valid = file.replace('train_', 'valid_')\n",
    "        \n",
    "        test_cat = os.path.basename(file).split('_')[0]\n",
    "        test = os.path.basename(file)\n",
    "        test = test.replace('.pkl.gz','_{}_gs.pkl.gz'.format(test_cat))\n",
    "        test ='../../../data/processed/wdc-lspc/wordcooc/training-sets-noised/{}'.format(test)\n",
    "        \n",
    "        run_wordcooc(file, valid, test, feature_combinations, classifiers, experiment_name, write_test_set_for_inspection=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
